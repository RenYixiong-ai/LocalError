{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目说明\n",
    "该项目复现local error训练，通过对深度网络的每一层单独计算准确性，最终实现整体的训练。\n",
    "\n",
    "这是一种新的网络训练方式，可以调控网络每一层的流形，从而实现可解释性的分析。\n",
    "\n",
    "文献参考：\n",
    "* [Deep Supervised Learning Using Local Errors](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2018.00608/full)\n",
    "* [Relationship between manifold smoothness and adversarial vulnerability in deep learning with local errors](https://cpb.iphy.ac.cn/EN/10.1088/1674-1056/abd68e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  数据输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义数据转换（将图像转换为Tensor并标准化）\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化（均值0.5，标准差0.5）\n",
    "])\n",
    "\n",
    "# 下载和加载训练集和测试集\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 查看训练数据的一部分\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape)  # 输出形状，应该是[64, 1, 28, 28]，即64张28x28的图像\n",
    "print(labels.shape)  # 输出标签形状，应该是[64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络构建\n",
    "\n",
    "单层网络和粘合多层网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SingleLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SingleLayerNetwork, self).__init__()\n",
    "        # 定义线性层\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        # 定义ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class MultiLayerNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()  # 用于存储逐步添加的网络层\n",
    "\n",
    "    def add(self, layer):\n",
    "        # 添加已训练好的网络层到ModuleList中\n",
    "        self.layers.append(copy.deepcopy(layer))\n",
    "\n",
    "\n",
    "    def forward(self, x, return_intermediate=False, n_layers=None):\n",
    "        outputs = []\n",
    "        \n",
    "        # 逐层计算输出\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if return_intermediate and (n_layers is None or i < n_layers):\n",
    "                outputs.append(x)\n",
    "        \n",
    "        if return_intermediate:\n",
    "            return outputs\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练方法\n",
    "通过读出头训练目标网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义读出头网络\n",
    "class ReadoutHead(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ReadoutHead, self).__init__()\n",
    "        # 初始化权重为高斯分布，且权重不可训练\n",
    "        self.weight = nn.Parameter(torch.randn(input_size, output_size) * 0.01, requires_grad=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 线性变换：y = xW + b\n",
    "        return torch.matmul(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练流程\n",
    "def train_with_readout(fixed_network, target_network, readout_head, data_loader, optimizer, criterion, device):\n",
    "    if fixed_network is not None:\n",
    "        fixed_network.eval()  # 固定网络不训练\n",
    "    target_network.train()  # 目标网络训练\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = inputs.view(inputs.shape[0], -1)  # 将图像展平\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 如果固定网络不为空，数据先通过固定网络（不计算梯度）\n",
    "        outputs = inputs\n",
    "        if fixed_network is not None:\n",
    "            with torch.no_grad():\n",
    "                outputs = fixed_network(inputs)\n",
    "\n",
    "\n",
    "        # 数据通过目标网络\n",
    "        target_outputs = target_network(outputs)\n",
    "\n",
    "        # 数据通过读出头网络\n",
    "        logits = readout_head(target_outputs)\n",
    "\n",
    "        # 计算交叉熵损失\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 反向传播优化目标网络\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_NN = MultiLayerNetwork()\n",
    "Single_NN = SingleLayerNetwork(28*28, 1000).to(device)\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(Single_NN.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "readout_head = ReadoutHead(1000, 10).to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train_with_readout(fixed_network=None, target_network=Single_NN, readout_head=readout_head, data_loader=trainloader, optimizer=optimizer, criterion=criterion, device=device)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(target_network, readout_head, data_loader, device):\n",
    "    # 固定网络、目标网络和读出头网络都设置为评估模式\n",
    "    target_network.eval()\n",
    "    readout_head.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # 将图像展平\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = inputs  # 否则直接使用原始输入\n",
    "\n",
    "            # 数据通过目标网络\n",
    "            target_outputs = target_network(outputs)\n",
    "\n",
    "            # 数据通过读出头网络\n",
    "            logits = readout_head(target_outputs)\n",
    "\n",
    "            # 预测类别\n",
    "            _, predicted = torch.max(logits, dim=1)  # 取概率最大的类别\n",
    "\n",
    "            # 统计正确预测的数量\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 计算并返回准确率\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "tot_NN.add(Single_NN)\n",
    "evaluate_accuracy(target_network=tot_NN, readout_head=readout_head, data_loader=testloader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建K层的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_NN = MultiLayerNetwork()\n",
    "input_size = 28*28\n",
    "size_range = [1000, 1000, 1000, 1000, 1000]\n",
    "for k, output_size in enumerate(size_range):\n",
    "    # 初始化一个单层网络\n",
    "    Single_NN = SingleLayerNetwork(input_size, output_size).to(device)\n",
    "    optimizer = optim.Adam(Single_NN.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    readout_head = ReadoutHead(output_size, 10).to(device)\n",
    "\n",
    "    # 训练该单层网络\n",
    "    for epoch in range(30):\n",
    "        loss = train_with_readout(fixed_network=tot_NN, target_network=Single_NN, readout_head=readout_head, data_loader=trainloader, optimizer=optimizer, criterion=criterion, device=device)\n",
    "        print(loss)\n",
    "\n",
    "    input_size = output_size\n",
    "    tot_NN.add(Single_NN)\n",
    "\n",
    "    eva_value = evaluate_accuracy(target_network=tot_NN, readout_head=readout_head, data_loader=testloader, device=device)\n",
    "    print(\"evsl\", eva_value)\n",
    "    \n",
    "final_eval = evaluate_accuracy(target_network=tot_NN, readout_head=readout_head, data_loader=testloader, device=device)\n",
    "print(final_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征指标\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算幂率指数$\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def estimate_alpha_mle(data, x_min):\n",
    "    \"\"\"\n",
    "    使用最大似然估计（MLE）计算幂律指数 α\n",
    "    :param data: 观测数据（numpy 数组）\n",
    "    :param x_min: 设定的最小阈值，幂律分布从 x_min 开始适用\n",
    "    :return: 估计的 α\n",
    "    \"\"\"\n",
    "    filtered_data = data[data >= x_min]  # 只选取大于等于 x_min 的数据\n",
    "    n = len(filtered_data)  # 数据点数\n",
    "    alpha = 1 + n / np.sum(np.log(filtered_data / x_min))\n",
    "    return alpha\n",
    "\n",
    "# 生成一个模拟的幂律分布数据\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "alpha_true = 2.5  # 真实幂律指数\n",
    "x_min = 1  # 设定最小阈值\n",
    "\n",
    "# 生成服从幂律分布的数据（使用逆变换采样法）\n",
    "random_values = np.random.uniform(size=n_samples)\n",
    "data = x_min * (1 - random_values) ** (-1 / (alpha_true - 1))\n",
    "\n",
    "# 估计幂律指数 α\n",
    "alpha_estimated = estimate_alpha_mle(data, x_min)\n",
    "print(f\"估计的幂律指数 α: {alpha_estimated:.4f}\")\n",
    "\n",
    "# 绘制直方图（对数-对数图）\n",
    "plt.figure(figsize=(8, 6))\n",
    "hist, bins, _ = plt.hist(data, bins=50, density=True, alpha=0.6, color='b', label='数据直方图')\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# 拟合一条直线（检查幂律特性）\n",
    "slope, intercept, _, _, _ = stats.linregress(np.log(bin_centers), np.log(hist))\n",
    "plt.plot(bin_centers, np.exp(intercept) * bin_centers**slope, 'r--', label=f'拟合: slope={slope:.2f}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('P(x)')\n",
    "plt.title('幂律分布的对数-对数图')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
