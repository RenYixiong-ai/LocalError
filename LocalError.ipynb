{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目说明\n",
    "该项目复现local error训练，通过对深度网络的每一层单独计算准确性，最终实现整体的训练。\n",
    "\n",
    "这是一种新的网络训练方式，可以调控网络每一层的流形，从而实现可解释性的分析。\n",
    "\n",
    "文献参考：\n",
    "* [Deep Supervised Learning Using Local Errors](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2018.00608/full)\n",
    "* [Relationship between manifold smoothness and adversarial vulnerability in deep learning with local errors](https://cpb.iphy.ac.cn/EN/10.1088/1674-1056/abd68e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  数据输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义数据转换（将图像转换为Tensor并标准化）\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化（均值0.5，标准差0.5）\n",
    "])\n",
    "\n",
    "# 下载和加载训练集和测试集\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 查看训练数据的一部分\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape)  # 输出形状，应该是[64, 1, 28, 28]，即64张28x28的图像\n",
    "print(labels.shape)  # 输出标签形状，应该是[64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络构建\n",
    "\n",
    "单层网络和粘合多层网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SingleLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SingleLayerNetwork, self).__init__()\n",
    "        # 定义线性层\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        # 定义ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class MultiLayerNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()  # 用于存储逐步添加的网络层\n",
    "\n",
    "    def add(self, layer):\n",
    "        # 添加已训练好的网络层到ModuleList中\n",
    "        self.layers.append(copy.deepcopy(layer))\n",
    "\n",
    "    def pop(self):\n",
    "        \"\"\"删除 self.layers 中的最后一个网络层\"\"\"\n",
    "        if len(self.layers) > 0:\n",
    "            last_layer = self.layers[-1]  # 获取最后一层\n",
    "            del self.layers[-1]  # 手动删除\n",
    "            return last_layer  # 返回被删除的层\n",
    "        else:\n",
    "            print(\"Warning: No layers to remove.\")\n",
    "            return None\n",
    "\n",
    "    def forward(self, x, n_layers=None, return_intermediate=False):\n",
    "        outputs = []\n",
    "        \n",
    "        # 逐层计算输出\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if return_intermediate and (n_layers is None or i < n_layers):\n",
    "                outputs.append(x)\n",
    "            if i == n_layers:\n",
    "                break\n",
    "        \n",
    "        if return_intermediate:\n",
    "            return outputs\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练方法\n",
    "通过读出头训练目标网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义读出头网络\n",
    "class ReadoutHead(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ReadoutHead, self).__init__()\n",
    "        # 初始化权重为高斯分布，且权重不可训练\n",
    "        self.weight = nn.Parameter(torch.randn(input_size, output_size) * 0.01, requires_grad=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 线性变换：y = xW + b\n",
    "        return torch.matmul(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练流程\n",
    "def train_with_readout(fixed_network, target_network, readout_head, data_loader, optimizer, criterion, device):\n",
    "    if fixed_network is not None:\n",
    "        fixed_network.eval()    # 固定网络不训练\n",
    "    target_network.train()      # 目标网络训练\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = inputs.view(inputs.shape[0], -1)  # 将图像展平\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 如果固定网络不为空，数据先通过固定网络（不计算梯度）\n",
    "        outputs = inputs\n",
    "        if fixed_network is not None:\n",
    "            with torch.no_grad():\n",
    "                outputs = fixed_network(inputs)\n",
    "\n",
    "\n",
    "        # 数据通过目标网络\n",
    "        target_outputs = target_network(outputs)\n",
    "\n",
    "        # 数据通过读出头网络\n",
    "        logits = readout_head(target_outputs)\n",
    "\n",
    "        # 计算交叉熵损失\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 反向传播优化目标网络\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_NN = MultiLayerNetwork()\n",
    "Single_NN = SingleLayerNetwork(28*28, 1000).to(device)\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(Single_NN.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "readout_head = ReadoutHead(1000, 10).to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = train_with_readout(fixed_network=None, target_network=Single_NN, readout_head=readout_head, data_loader=trainloader, optimizer=optimizer, criterion=criterion, device=device)\n",
    "    print(loss)\n",
    "\n",
    "tot_NN.add(Single_NN)\n",
    "tot_NN.add(readout_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(target_network, data_loader, device, readout_head=None):\n",
    "    # 固定网络、目标网络和读出头网络都设置为评估模式\n",
    "    target_network.eval()\n",
    "    if readout_head is not None:\n",
    "        readout_head.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # 将图像展平\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 数据通过目标网络\n",
    "            target_outputs = target_network(inputs)\n",
    "\n",
    "            if readout_head is not None:\n",
    "                # 数据通过读出头网络\n",
    "                target_outputs = readout_head(target_outputs)\n",
    "\n",
    "            # 预测类别\n",
    "            _, predicted = torch.max(target_outputs, dim=1)  # 取概率最大的类别\n",
    "\n",
    "            # 统计正确预测的数量\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 计算并返回准确率\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "evaluate_accuracy(target_network=tot_NN, readout_head=None, data_loader=testloader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建K层的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_NN = MultiLayerNetwork()\n",
    "input_size = 28*28\n",
    "size_range = [1000, 1000, 1000, 1000]\n",
    "for k, output_size in enumerate(size_range):\n",
    "    # 初始化一个单层网络\n",
    "    Single_NN = SingleLayerNetwork(input_size, output_size).to(device)\n",
    "    optimizer = optim.Adam(Single_NN.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    readout_head = ReadoutHead(output_size, 10).to(device)\n",
    "\n",
    "    # 训练该单层网络\n",
    "    for epoch in range(3):\n",
    "        loss = train_with_readout(fixed_network=tot_NN, target_network=Single_NN, readout_head=readout_head, data_loader=trainloader, optimizer=optimizer, criterion=criterion, device=device)\n",
    "        print(loss)\n",
    "\n",
    "    input_size = output_size\n",
    "    tot_NN.add(Single_NN)\n",
    "\n",
    "    eva_value = evaluate_accuracy(target_network=tot_NN, readout_head=readout_head, data_loader=testloader, device=device)\n",
    "    print(\"evsl\", eva_value)\n",
    "\n",
    "tot_NN.add(readout_head)\n",
    "\n",
    "final_eval = evaluate_accuracy(target_network=tot_NN, data_loader=testloader, device=device)\n",
    "print(final_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征指标\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算幂率指数$\\alpha$\n",
    "\n",
    "幂律分布的概率密度函数（PDF）一般形式为：\n",
    "\n",
    "$$P(x) \\propto x^{-\\alpha}$$\n",
    "\n",
    "或者更具体地写成：\n",
    "\n",
    "$$P(x) = C x^{-\\alpha}, \\quad x \\geq x_{\\min}$$\n",
    "\n",
    "其中：\n",
    "- $x$ 是一个随机变量（如网络节点的度、地震震级、财富分布等）。\n",
    "- $\\alpha$ 是幂律指数（Power-Law Exponent），决定分布的陡峭程度。\n",
    "- $x_{\\min}$ 是幂律分布的最小适用值，在某些数据中，幂律分布可能只适用于某个范围以上的值。\n",
    "- $C$ 是归一化常数，使得概率总和为 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def estimate_alpha_mle(data, x_min):\n",
    "    \"\"\"\n",
    "    使用最大似然估计（MLE）计算幂律指数 α\n",
    "    :param data: 观测数据（numpy 数组）\n",
    "    :param x_min: 设定的最小阈值，幂律分布从 x_min 开始适用\n",
    "    :return: 估计的 α\n",
    "    \"\"\"\n",
    "    filtered_data = data[data >= x_min]  # 只选取大于等于 x_min 的数据\n",
    "    n = len(filtered_data)  # 数据点数\n",
    "    alpha = 1 + n / np.sum(np.log(filtered_data / x_min))\n",
    "    return alpha\n",
    "\n",
    "# 生成一个模拟的幂律分布数据\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "alpha_true = 2.5  # 真实幂律指数\n",
    "x_min = 1  # 设定最小阈值\n",
    "\n",
    "# 生成服从幂律分布的数据（使用逆变换采样法）\n",
    "random_values = np.random.uniform(size=n_samples)\n",
    "data = x_min * (1 - random_values) ** (-1 / (alpha_true - 1))\n",
    "\n",
    "# 估计幂律指数 α\n",
    "alpha_estimated = estimate_alpha_mle(data, x_min)\n",
    "print(f\"估计的幂律指数 α: {alpha_estimated:.4f}\")\n",
    "\n",
    "# 绘制直方图（对数-对数图）\n",
    "plt.figure(figsize=(8, 6))\n",
    "hist, bins, _ = plt.hist(data, bins=500, density=True, alpha=0.6, color='b')\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# 过滤掉 hist == 0 的数据点，避免 log(0) 错误\n",
    "valid_indices = hist > 0\n",
    "log_bin_centers = np.log(bin_centers[valid_indices])\n",
    "log_hist = np.log(hist[valid_indices])\n",
    "\n",
    "# 线性回归（拟合幂律指数）\n",
    "_, intercept, _, _, _ = stats.linregress(log_bin_centers, log_hist)\n",
    "slope = -alpha_estimated\n",
    "plt.plot(bin_centers, np.exp(intercept) * bin_centers**slope, 'r--', label=f'fit: slope={slope:.2f}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('P(x)')\n",
    "plt.title('log-log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算决定系数\n",
    "用于衡量 **主成分保留的方差信息**。\n",
    "\n",
    "在 **回归分析** 中，$ R^2 $ 衡量模型对数据的解释能力：\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "$$\n",
    "其中：\n",
    "- $ y_i $ 是真实值，\n",
    "- $ \\hat{y}_i $ 是模型预测值，\n",
    "- $ \\bar{y} $ 是 $ y $ 的均值。\n",
    "\n",
    "在 PCA 中，类似的概念是：\n",
    "$$\n",
    "R^2 = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i}\n",
    "$$\n",
    "其中：\n",
    "- $ \\lambda_i $ 是第 $ i $ 个特征值（主成分的方差）。\n",
    "- $ k $ 是选取的前 $ k $ 个主成分。\n",
    "- $ d $ 是所有特征维度（在你的例子中 $ d = 1000 $）。\n",
    "\n",
    "这个公式表示 **前 $ k $ 个主成分解释了多少比例的总方差**，即 **主成分的累计方差贡献率**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eigenvalues = [3.2, 2.9, 2.5, 1.8, 1.5, 1.2, 0.9, 0.7, 0.5, 0.3]\n",
    "\n",
    "# 假设 eigenvalues 是特征值数组 (已按降序排列)\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)  # 计算解释方差比例\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)  # 计算累计贡献率\n",
    "\n",
    "# 选择前 k 个主成分，使得累计贡献率达到 95%\n",
    "k = np.argmax(cumulative_variance >= 0.95) + 1  # 找到累计方差贡献率 >= 95% 的最小维度\n",
    "R2_95 = cumulative_variance[k - 1]\n",
    "\n",
    "print(f\"To retain 95% variance, we need {k} principal components.\")\n",
    "print(f\"R^2 for k={k} components: {R2_95:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络信息处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tot_NN.eval()  # 设为评估模式，不启用 Dropout、BatchNorm\n",
    "output_list = []\n",
    "\n",
    "with torch.no_grad():  # 不计算梯度，加速推理\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs = inputs.view(inputs.shape[0], -1)  # 将图像展平\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = tot_NN(inputs, 3)  # 前向传播\n",
    "        output_list.append(output.cpu().numpy())  # 转换为 NumPy 并保存\n",
    "\n",
    "# 拼接成一个完整的 NumPy 数组\n",
    "final_output = np.vstack(output_list) \n",
    "final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eigenvalues(data):\n",
    "    data = data - np.mean(data, axis=0)\n",
    "    # 计算数据的协方差矩阵\n",
    "    covariance_matrix = np.cov(data, rowvar=False)\n",
    "    \n",
    "    # 计算协方差矩阵的特征值\n",
    "    eigenvalues, _ = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    # 对特征值进行排序\n",
    "    eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "    return eigenvalues\n",
    "\n",
    "def get_alpha_r(eigenvalues):\n",
    "    # 估计幂律指数 α\n",
    "    slope = estimate_alpha_mle(eigenvalues, 0.000001)\n",
    "\n",
    "    # 计算R^2\n",
    "    # 假设 eigenvalues 是特征值数组 (已按降序排列)\n",
    "    explained_variance_ratio = eigenvalues / np.sum(eigenvalues)  # 计算解释方差比例\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)  # 计算累计贡献率\n",
    "\n",
    "    # 选择前 k 个主成分，使得累计贡献率达到 95%\n",
    "    k = np.argmax(cumulative_variance >= 0.95) + 1  # 找到累计方差贡献率 >= 95% 的最小维度\n",
    "    R2_95 = cumulative_variance[10]\n",
    "    return slope, R2_95, k\n",
    "\n",
    "eigenvalues = get_eigenvalues(final_output)\n",
    "slope, R2_95, k = get_alpha_r(eigenvalues)\n",
    "\n",
    "print(slope)\n",
    "print(f\"To retain 95% variance, we need {k} principal components.\")\n",
    "print(f\"R^2 for k={k} components: {R2_95:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
